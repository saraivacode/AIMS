================================================================================
AIMS Framework - Training Pipeline
================================================================================

[1/3] Training Random Forest Classifier...
------------------------------------------------------------

================================================================================
RANDOM FOREST TRAINING PIPELINE
================================================================================

Saving preprocessing artifacts...
‚úì Features saved: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/rf/X_rf.csv
‚úì Labels saved: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/rf/y_rf.csv (text) and y_rf.npy (binary)
‚úì Groups saved: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/rf/groups_rf.csv (text) and groups_rf.npy (binary)
‚úì Class weights saved: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/rf/class_weight_rf.json

üìÅ All artifacts saved to: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/rf

------------------------------------------------------------
HYPERPARAMETER OPTIMIZATION
------------------------------------------------------------
Best‚Äátrial:‚Äá16.‚ÄáBest‚Äávalue:‚Äá0.963688:‚Äá100%
‚Äá40/40‚Äá[37:24<00:00,‚Äá47.70s/it]

Optimization completed!
Best parameters: {'n_estimators': 300, 'max_depth': None, 'min_samples_leaf': 2, 'max_features': 0.5}
Best F1-macro (CV): 0.964

------------------------------------------------------------
FINAL MODEL TRAINING
------------------------------------------------------------
‚úì Model saved: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/rf/rf_best.pkl

------------------------------------------------------------
HOLDOUT SET EVALUATION
------------------------------------------------------------

Classification Report:
              precision    recall  f1-score   support

           0      0.994     0.883     0.935       196
           1      0.987     0.939     0.963       329
           2      0.919     0.990     0.953       492
           3      0.979     0.979     0.979       141

    accuracy                          0.956      1158
   macro avg      0.970     0.948     0.957      1158
weighted avg      0.958     0.956     0.956      1158

‚úì Results saved: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/rf/training_results_rf.json

------------------------------------------------------------
GENERATING VISUALIZATIONS
------------------------------------------------------------
‚úì Confusion matrices saved
‚úì Optimization history saved: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/rf/rf_optuna_history.html

------------------------------------------------------------
FEATURE IMPORTANCE ANALYSIS
------------------------------------------------------------

Top 10 Most Important Features:
num__pdr                      : 0.2660
num__loss_ratio               : 0.1830
num__lat_ms                   : 0.1531
num__vazao_rec_servidor_media_bps: 0.1122
num__lat_ms_mean3             : 0.0873
num__pdr_mean3                : 0.0511
num__lambda                   : 0.0317
cat__arquivo_origem_metricas_v10_ci_g_fn.csv: 0.0166
num__thr_util                 : 0.0117
num__throughput_kbps          : 0.0109

‚úì Feature importances saved: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/rf/feature_importances_rf.csv

================================================================================
RANDOM FOREST TRAINING COMPLETED SUCCESSFULLY!
================================================================================
‚úì Random Forest training completed

[2/3] Training TabNet Classifier...
------------------------------------------------------------

================================================================================
TABNET TRAINING PIPELINE
================================================================================

[Step 1/9] Loading and preprocessing dataset...
‚úì Dataset shape: (5400, 28)
‚úì Impact distribution:
0     768
1    1013
2    2715
3     904
Name: count, dtype: int64
‚úì Class weights: {0: 1.7578125, 1: 1.332675222112537, 2: 0.4972375690607735, 3: 1.4933628318584071}

Saving preprocessing artifacts for TabNet...
‚úì Features saved: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/tabnet/X_tabnet.csv
‚úì Labels saved: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/tabnet/y_tabnet.csv (text) and y_tabnet.npy (binary)
‚úì Groups saved: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/tabnet/groups_tabnet.csv (text) and groups_tabnet.npy (binary)
‚úì Class weights saved: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/tabnet/class_weight_tabnet.json

üìÅ All artifacts saved to: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/tabnet

[Step 2/9] Preparing features for TabNet...
‚úì Numerical features (21): ['n_carros', 'vazao_env_carro_total_bps', 'vazao_env_carro_media_bps', 'throughput_bps', 'vazao_rec_servidor_media_bps']...
‚úì Categorical features (7): ['timestamp_sec_str', 'app', 'cenario', 'arquivo_origem', 'categoria', 'timestamp', 'group_id']
‚úì Categorical indices: [21, 22, 23, 24, 25, 26, 27]
‚úì Categorical dimensions: [462, 4, 3, 12, 4, 462, 32]

[Step 3/9] Creating temporal-aware data splits...
‚úì Training set: 4242 samples (78.6%)
‚úì Holdout set: 1158 samples (21.4%)

[Step 4/9] Configuring computing device...
‚úì GPU detected: Tesla T4

[Step 5/9] Starting hyperparameter optimization with Optuna...
------------------------------------------------------------
Best‚Äátrial:‚Äá10.‚ÄáBest‚Äávalue:‚Äá0.931255:‚Äá100%
‚Äá20/20‚Äá[16:18<00:00,‚Äá55.36s/it]

Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_accuracy = 0.91892
Stop training because you reached max_epochs = 50 with best_epoch = 45 and best_val_0_accuracy = 0.91667

Early stopping occurred at epoch 37 with best_epoch = 17 and best_val_0_accuracy = 0.82117

Early stopping occurred at epoch 22 with best_epoch = 2 and best_val_0_accuracy = 0.80535

Early stopping occurred at epoch 29 with best_epoch = 9 and best_val_0_accuracy = 0.8747

Early stopping occurred at epoch 24 with best_epoch = 4 and best_val_0_accuracy = 0.48874

Early stopping occurred at epoch 22 with best_epoch = 2 and best_val_0_accuracy = 0.59459
Stop training because you reached max_epochs = 50 with best_epoch = 47 and best_val_0_accuracy = 0.77372
Stop training because you reached max_epochs = 50 with best_epoch = 49 and best_val_0_accuracy = 0.81752
Stop training because you reached max_epochs = 50 with best_epoch = 48 and best_val_0_accuracy = 0.88321
Stop training because you reached max_epochs = 50 with best_epoch = 48 and best_val_0_accuracy = 0.88176
Stop training because you reached max_epochs = 50 with best_epoch = 30 and best_val_0_accuracy = 0.88063
Stop training because you reached max_epochs = 50 with best_epoch = 46 and best_val_0_accuracy = 0.90268
Stop training because you reached max_epochs = 50 with best_epoch = 41 and best_val_0_accuracy = 0.9635
Stop training because you reached max_epochs = 50 with best_epoch = 47 and best_val_0_accuracy = 0.96715

Early stopping occurred at epoch 35 with best_epoch = 15 and best_val_0_accuracy = 0.86261

Early stopping occurred at epoch 49 with best_epoch = 29 and best_val_0_accuracy = 0.89977

Early stopping occurred at epoch 31 with best_epoch = 11 and best_val_0_accuracy = 0.85766

Early stopping occurred at epoch 26 with best_epoch = 6 and best_val_0_accuracy = 0.87591
Stop training because you reached max_epochs = 50 with best_epoch = 49 and best_val_0_accuracy = 0.91119
Stop training because you reached max_epochs = 50 with best_epoch = 46 and best_val_0_accuracy = 0.76577
Stop training because you reached max_epochs = 50 with best_epoch = 45 and best_val_0_accuracy = 0.86261
Stop training because you reached max_epochs = 50 with best_epoch = 49 and best_val_0_accuracy = 0.78224
Stop training because you reached max_epochs = 50 with best_epoch = 42 and best_val_0_accuracy = 0.91849
Stop training because you reached max_epochs = 50 with best_epoch = 45 and best_val_0_accuracy = 0.91849

Early stopping occurred at epoch 38 with best_epoch = 18 and best_val_0_accuracy = 0.81532
Stop training because you reached max_epochs = 50 with best_epoch = 42 and best_val_0_accuracy = 0.89189
Stop training because you reached max_epochs = 50 with best_epoch = 33 and best_val_0_accuracy = 0.85158
Stop training because you reached max_epochs = 50 with best_epoch = 45 and best_val_0_accuracy = 0.94526

Early stopping occurred at epoch 29 with best_epoch = 9 and best_val_0_accuracy = 0.88564
Stop training because you reached max_epochs = 50 with best_epoch = 34 and best_val_0_accuracy = 0.84797
Stop training because you reached max_epochs = 50 with best_epoch = 30 and best_val_0_accuracy = 0.87725
Stop training because you reached max_epochs = 50 with best_epoch = 41 and best_val_0_accuracy = 0.84185

Early stopping occurred at epoch 41 with best_epoch = 21 and best_val_0_accuracy = 0.91363

Early stopping occurred at epoch 37 with best_epoch = 17 and best_val_0_accuracy = 0.8601
Stop training because you reached max_epochs = 50 with best_epoch = 47 and best_val_0_accuracy = 0.83896
Stop training because you reached max_epochs = 50 with best_epoch = 49 and best_val_0_accuracy = 0.88851
Stop training because you reached max_epochs = 50 with best_epoch = 49 and best_val_0_accuracy = 0.84185

Early stopping occurred at epoch 36 with best_epoch = 16 and best_val_0_accuracy = 0.80779

Early stopping occurred at epoch 49 with best_epoch = 29 and best_val_0_accuracy = 0.88078
Stop training because you reached max_epochs = 50 with best_epoch = 49 and best_val_0_accuracy = 0.86374
Stop training because you reached max_epochs = 50 with best_epoch = 36 and best_val_0_accuracy = 0.83446
Stop training because you reached max_epochs = 50 with best_epoch = 41 and best_val_0_accuracy = 0.81265

Early stopping occurred at epoch 34 with best_epoch = 14 and best_val_0_accuracy = 0.75182

Early stopping occurred at epoch 47 with best_epoch = 27 and best_val_0_accuracy = 0.68491
Stop training because you reached max_epochs = 50 with best_epoch = 41 and best_val_0_accuracy = 0.76914
Stop training because you reached max_epochs = 50 with best_epoch = 34 and best_val_0_accuracy = 0.83671
Stop training because you reached max_epochs = 50 with best_epoch = 47 and best_val_0_accuracy = 0.73844
Stop training because you reached max_epochs = 50 with best_epoch = 47 and best_val_0_accuracy = 0.90754
Stop training because you reached max_epochs = 50 with best_epoch = 49 and best_val_0_accuracy = 0.88078
Stop training because you reached max_epochs = 50 with best_epoch = 49 and best_val_0_accuracy = 0.93018

Early stopping occurred at epoch 42 with best_epoch = 22 and best_val_0_accuracy = 0.91329
Stop training because you reached max_epochs = 50 with best_epoch = 44 and best_val_0_accuracy = 0.88686
Stop training because you reached max_epochs = 50 with best_epoch = 49 and best_val_0_accuracy = 0.98297
Stop training because you reached max_epochs = 50 with best_epoch = 48 and best_val_0_accuracy = 0.98054
Stop training because you reached max_epochs = 50 with best_epoch = 30 and best_val_0_accuracy = 0.89977
Stop training because you reached max_epochs = 50 with best_epoch = 48 and best_val_0_accuracy = 0.91892
Stop training because you reached max_epochs = 50 with best_epoch = 43 and best_val_0_accuracy = 0.89173
Stop training because you reached max_epochs = 50 with best_epoch = 43 and best_val_0_accuracy = 0.98297
Stop training because you reached max_epochs = 50 with best_epoch = 49 and best_val_0_accuracy = 0.95012
Stop training because you reached max_epochs = 50 with best_epoch = 46 and best_val_0_accuracy = 0.90991
Stop training because you reached max_epochs = 50 with best_epoch = 48 and best_val_0_accuracy = 0.93018
Stop training because you reached max_epochs = 50 with best_epoch = 36 and best_val_0_accuracy = 0.882
Stop training because you reached max_epochs = 50 with best_epoch = 40 and best_val_0_accuracy = 0.94769

Early stopping occurred at epoch 41 with best_epoch = 21 and best_val_0_accuracy = 0.92214
Stop training because you reached max_epochs = 50 with best_epoch = 49 and best_val_0_accuracy = 0.77928
Stop training because you reached max_epochs = 50 with best_epoch = 45 and best_val_0_accuracy = 0.8705
Stop training because you reached max_epochs = 50 with best_epoch = 45 and best_val_0_accuracy = 0.7871
Stop training because you reached max_epochs = 50 with best_epoch = 48 and best_val_0_accuracy = 0.90633
Stop training because you reached max_epochs = 50 with best_epoch = 40 and best_val_0_accuracy = 0.89416
Stop training because you reached max_epochs = 50 with best_epoch = 39 and best_val_0_accuracy = 0.89077
Stop training because you reached max_epochs = 50 with best_epoch = 39 and best_val_0_accuracy = 0.9009
Stop training because you reached max_epochs = 50 with best_epoch = 44 and best_val_0_accuracy = 0.88808
Stop training because you reached max_epochs = 50 with best_epoch = 48 and best_val_0_accuracy = 0.95985
Stop training because you reached max_epochs = 50 with best_epoch = 47 and best_val_0_accuracy = 0.94161
Stop training because you reached max_epochs = 50 with best_epoch = 48 and best_val_0_accuracy = 0.85586

Early stopping occurred at epoch 39 with best_epoch = 19 and best_val_0_accuracy = 0.81982
Stop training because you reached max_epochs = 50 with best_epoch = 49 and best_val_0_accuracy = 0.87835
Stop training because you reached max_epochs = 50 with best_epoch = 47 and best_val_0_accuracy = 0.87105
Stop training because you reached max_epochs = 50 with best_epoch = 47 and best_val_0_accuracy = 0.88443
Stop training because you reached max_epochs = 50 with best_epoch = 36 and best_val_0_accuracy = 0.89865
Stop training because you reached max_epochs = 50 with best_epoch = 48 and best_val_0_accuracy = 0.91554

Early stopping occurred at epoch 41 with best_epoch = 21 and best_val_0_accuracy = 0.89051
Stop training because you reached max_epochs = 50 with best_epoch = 47 and best_val_0_accuracy = 0.94526
Stop training because you reached max_epochs = 50 with best_epoch = 47 and best_val_0_accuracy = 0.9635
Stop training because you reached max_epochs = 50 with best_epoch = 47 and best_val_0_accuracy = 0.44595
Stop training because you reached max_epochs = 50 with best_epoch = 49 and best_val_0_accuracy = 0.49324
Stop training because you reached max_epochs = 50 with best_epoch = 49 and best_val_0_accuracy = 0.44161
Stop training because you reached max_epochs = 50 with best_epoch = 49 and best_val_0_accuracy = 0.68856
Stop training because you reached max_epochs = 50 with best_epoch = 48 and best_val_0_accuracy = 0.44769
Stop training because you reached max_epochs = 50 with best_epoch = 49 and best_val_0_accuracy = 0.92455

Early stopping occurred at epoch 35 with best_epoch = 15 and best_val_0_accuracy = 0.88288
Stop training because you reached max_epochs = 50 with best_epoch = 34 and best_val_0_accuracy = 0.91119
Stop training because you reached max_epochs = 50 with best_epoch = 42 and best_val_0_accuracy = 0.96715
Stop training because you reached max_epochs = 50 with best_epoch = 33 and best_val_0_accuracy = 0.9635
Stop training because you reached max_epochs = 50 with best_epoch = 41 and best_val_0_accuracy = 0.89527
Stop training because you reached max_epochs = 50 with best_epoch = 38 and best_val_0_accuracy = 0.90541
Stop training because you reached max_epochs = 50 with best_epoch = 46 and best_val_0_accuracy = 0.85401
Stop training because you reached max_epochs = 50 with best_epoch = 41 and best_val_0_accuracy = 0.93431
Stop training because you reached max_epochs = 50 with best_epoch = 48 and best_val_0_accuracy = 0.96594

‚úì Optimization completed!
Best parameters found:
  n_d: 16
  n_a: 8
  n_steps: 5
  gamma: 1.0
  lambda_sparse: 0.0002145939700681214
  lr: 0.02191446947212663
  mask_type: sparsemax
Best F1-macro (CV): 0.931

[Step 6/9] Training final TabNet model with best parameters...

Training with early stopping (patience=30)...
epoch 0  | loss: 2.82612 | val_0_accuracy: 0.31261 |  0:00:00s
epoch 1  | loss: 1.16562 | val_0_accuracy: 0.54231 |  0:00:00s
epoch 2  | loss: 0.8763  | val_0_accuracy: 0.5924  |  0:00:01s
epoch 3  | loss: 0.64603 | val_0_accuracy: 0.65717 |  0:00:01s
epoch 4  | loss: 0.49315 | val_0_accuracy: 0.70121 |  0:00:01s
epoch 5  | loss: 0.39891 | val_0_accuracy: 0.77461 |  0:00:01s
epoch 6  | loss: 0.33542 | val_0_accuracy: 0.82124 |  0:00:02s
epoch 7  | loss: 0.30697 | val_0_accuracy: 0.82211 |  0:00:02s
epoch 8  | loss: 0.28286 | val_0_accuracy: 0.82556 |  0:00:02s
epoch 9  | loss: 0.28101 | val_0_accuracy: 0.82297 |  0:00:03s
epoch 10 | loss: 0.27404 | val_0_accuracy: 0.83592 |  0:00:03s
epoch 11 | loss: 0.27818 | val_0_accuracy: 0.82124 |  0:00:03s
epoch 12 | loss: 0.23941 | val_0_accuracy: 0.83247 |  0:00:04s
epoch 13 | loss: 0.2264  | val_0_accuracy: 0.84283 |  0:00:04s
epoch 14 | loss: 0.2409  | val_0_accuracy: 0.84111 |  0:00:05s
epoch 15 | loss: 0.19822 | val_0_accuracy: 0.83765 |  0:00:05s
epoch 16 | loss: 0.20413 | val_0_accuracy: 0.84456 |  0:00:06s
epoch 17 | loss: 0.18314 | val_0_accuracy: 0.82642 |  0:00:06s
epoch 18 | loss: 0.17962 | val_0_accuracy: 0.81865 |  0:00:07s
epoch 19 | loss: 0.1625  | val_0_accuracy: 0.84456 |  0:00:07s
epoch 20 | loss: 0.16305 | val_0_accuracy: 0.85147 |  0:00:07s
epoch 21 | loss: 0.18033 | val_0_accuracy: 0.84629 |  0:00:08s
epoch 22 | loss: 0.15521 | val_0_accuracy: 0.86701 |  0:00:08s
epoch 23 | loss: 0.13598 | val_0_accuracy: 0.85492 |  0:00:08s
epoch 24 | loss: 0.12774 | val_0_accuracy: 0.83161 |  0:00:08s
epoch 25 | loss: 0.12918 | val_0_accuracy: 0.8791  |  0:00:09s
epoch 26 | loss: 0.14527 | val_0_accuracy: 0.86528 |  0:00:09s
epoch 27 | loss: 0.11238 | val_0_accuracy: 0.87737 |  0:00:09s
epoch 28 | loss: 0.13973 | val_0_accuracy: 0.88774 |  0:00:10s
epoch 29 | loss: 0.13405 | val_0_accuracy: 0.86097 |  0:00:10s
epoch 30 | loss: 0.12324 | val_0_accuracy: 0.85233 |  0:00:10s
epoch 31 | loss: 0.11629 | val_0_accuracy: 0.85924 |  0:00:11s
epoch 32 | loss: 0.10957 | val_0_accuracy: 0.87392 |  0:00:11s
epoch 33 | loss: 0.12659 | val_0_accuracy: 0.85233 |  0:00:11s
epoch 34 | loss: 0.159   | val_0_accuracy: 0.87651 |  0:00:12s
epoch 35 | loss: 0.13062 | val_0_accuracy: 0.88515 |  0:00:12s
epoch 36 | loss: 0.12837 | val_0_accuracy: 0.87565 |  0:00:12s
epoch 37 | loss: 0.10824 | val_0_accuracy: 0.85233 |  0:00:12s
epoch 38 | loss: 0.11081 | val_0_accuracy: 0.89033 |  0:00:13s
epoch 39 | loss: 0.09969 | val_0_accuracy: 0.84715 |  0:00:13s
epoch 40 | loss: 0.11176 | val_0_accuracy: 0.88687 |  0:00:13s
epoch 41 | loss: 0.09974 | val_0_accuracy: 0.87133 |  0:00:14s
epoch 42 | loss: 0.09958 | val_0_accuracy: 0.87478 |  0:00:14s
epoch 43 | loss: 0.09473 | val_0_accuracy: 0.82642 |  0:00:14s
epoch 44 | loss: 0.09237 | val_0_accuracy: 0.87824 |  0:00:15s
epoch 45 | loss: 0.09957 | val_0_accuracy: 0.86097 |  0:00:15s
epoch 46 | loss: 0.09742 | val_0_accuracy: 0.87392 |  0:00:15s
epoch 47 | loss: 0.09643 | val_0_accuracy: 0.87824 |  0:00:16s
epoch 48 | loss: 0.09632 | val_0_accuracy: 0.88428 |  0:00:16s
epoch 49 | loss: 0.09135 | val_0_accuracy: 0.84542 |  0:00:16s
epoch 50 | loss: 0.09128 | val_0_accuracy: 0.88515 |  0:00:16s
epoch 51 | loss: 0.10213 | val_0_accuracy: 0.81779 |  0:00:17s
epoch 52 | loss: 0.08926 | val_0_accuracy: 0.87306 |  0:00:17s
epoch 53 | loss: 0.11687 | val_0_accuracy: 0.87392 |  0:00:18s
epoch 54 | loss: 0.09089 | val_0_accuracy: 0.87565 |  0:00:18s
epoch 55 | loss: 0.10935 | val_0_accuracy: 0.89465 |  0:00:19s
epoch 56 | loss: 0.09607 | val_0_accuracy: 0.83938 |  0:00:19s
epoch 57 | loss: 0.10829 | val_0_accuracy: 0.90328 |  0:00:19s
epoch 58 | loss: 0.0908  | val_0_accuracy: 0.89033 |  0:00:20s
epoch 59 | loss: 0.08397 | val_0_accuracy: 0.88601 |  0:00:20s
epoch 60 | loss: 0.08432 | val_0_accuracy: 0.83938 |  0:00:21s
epoch 61 | loss: 0.08084 | val_0_accuracy: 0.87651 |  0:00:21s
epoch 62 | loss: 0.08234 | val_0_accuracy: 0.86528 |  0:00:21s
epoch 63 | loss: 0.08861 | val_0_accuracy: 0.87651 |  0:00:21s
epoch 64 | loss: 0.0865  | val_0_accuracy: 0.85147 |  0:00:22s
epoch 65 | loss: 0.0776  | val_0_accuracy: 0.90242 |  0:00:22s
epoch 66 | loss: 0.08606 | val_0_accuracy: 0.88601 |  0:00:22s
epoch 67 | loss: 0.07952 | val_0_accuracy: 0.88946 |  0:00:23s
epoch 68 | loss: 0.08959 | val_0_accuracy: 0.87047 |  0:00:23s
epoch 69 | loss: 0.07126 | val_0_accuracy: 0.90155 |  0:00:23s
epoch 70 | loss: 0.08077 | val_0_accuracy: 0.91364 |  0:00:24s
epoch 71 | loss: 0.08446 | val_0_accuracy: 0.88256 |  0:00:24s
epoch 72 | loss: 0.08366 | val_0_accuracy: 0.89033 |  0:00:24s
epoch 73 | loss: 0.083   | val_0_accuracy: 0.86269 |  0:00:25s
epoch 74 | loss: 0.07993 | val_0_accuracy: 0.8791  |  0:00:25s
epoch 75 | loss: 0.08633 | val_0_accuracy: 0.90501 |  0:00:25s
epoch 76 | loss: 0.07587 | val_0_accuracy: 0.90415 |  0:00:25s
epoch 77 | loss: 0.08078 | val_0_accuracy: 0.90415 |  0:00:26s
epoch 78 | loss: 0.07953 | val_0_accuracy: 0.91969 |  0:00:26s
epoch 79 | loss: 0.08175 | val_0_accuracy: 0.90587 |  0:00:26s
epoch 80 | loss: 0.06187 | val_0_accuracy: 0.84456 |  0:00:27s
epoch 81 | loss: 0.06972 | val_0_accuracy: 0.92314 |  0:00:27s
epoch 82 | loss: 0.05502 | val_0_accuracy: 0.88169 |  0:00:27s
epoch 83 | loss: 0.0557  | val_0_accuracy: 0.89896 |  0:00:28s
epoch 84 | loss: 0.0553  | val_0_accuracy: 0.85665 |  0:00:28s
epoch 85 | loss: 0.06134 | val_0_accuracy: 0.89551 |  0:00:28s
epoch 86 | loss: 0.06288 | val_0_accuracy: 0.89551 |  0:00:29s
epoch 87 | loss: 0.05665 | val_0_accuracy: 0.87219 |  0:00:29s
epoch 88 | loss: 0.06047 | val_0_accuracy: 0.90155 |  0:00:29s
epoch 89 | loss: 0.05457 | val_0_accuracy: 0.87047 |  0:00:30s
epoch 90 | loss: 0.05481 | val_0_accuracy: 0.91364 |  0:00:30s
epoch 91 | loss: 0.05346 | val_0_accuracy: 0.86442 |  0:00:30s
epoch 92 | loss: 0.05637 | val_0_accuracy: 0.90155 |  0:00:31s
epoch 93 | loss: 0.04935 | val_0_accuracy: 0.86874 |  0:00:31s
epoch 94 | loss: 0.0496  | val_0_accuracy: 0.87047 |  0:00:32s
epoch 95 | loss: 0.0506  | val_0_accuracy: 0.86788 |  0:00:32s
epoch 96 | loss: 0.05373 | val_0_accuracy: 0.87478 |  0:00:33s
epoch 97 | loss: 0.04971 | val_0_accuracy: 0.86615 |  0:00:33s
epoch 98 | loss: 0.05223 | val_0_accuracy: 0.87824 |  0:00:34s
epoch 99 | loss: 0.06786 | val_0_accuracy: 0.8696  |  0:00:34s
epoch 100| loss: 0.04999 | val_0_accuracy: 0.88687 |  0:00:34s
epoch 101| loss: 0.05671 | val_0_accuracy: 0.89983 |  0:00:34s
epoch 102| loss: 0.04847 | val_0_accuracy: 0.9076  |  0:00:35s
epoch 103| loss: 0.05296 | val_0_accuracy: 0.88083 |  0:00:35s
epoch 104| loss: 0.04023 | val_0_accuracy: 0.86615 |  0:00:35s
epoch 105| loss: 0.04285 | val_0_accuracy: 0.88946 |  0:00:36s
epoch 106| loss: 0.04113 | val_0_accuracy: 0.86356 |  0:00:36s
epoch 107| loss: 0.04495 | val_0_accuracy: 0.87565 |  0:00:36s
epoch 108| loss: 0.04763 | val_0_accuracy: 0.87306 |  0:00:37s
epoch 109| loss: 0.05488 | val_0_accuracy: 0.86356 |  0:00:37s
epoch 110| loss: 0.04265 | val_0_accuracy: 0.85924 |  0:00:37s
epoch 111| loss: 0.04801 | val_0_accuracy: 0.85924 |  0:00:37s

Early stopping occurred at epoch 111 with best_epoch = 81 and best_val_0_accuracy = 0.92314

‚úì Model saved: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/tabnet/tabnet_best.pkl

[Step 7/9] Evaluating on holdout set...
------------------------------------------------------------

Classification Report:
              precision    recall  f1-score   support

           0      0.984     0.964     0.974       196
           1      0.845     0.930     0.886       329
           2      0.942     0.894     0.918       492
           3      0.978     0.950     0.964       141

    accuracy                          0.923      1158
   macro avg      0.937     0.935     0.935      1158
weighted avg      0.926     0.923     0.924      1158


Key Performance Indicators:
  ‚Ä¢ Overall Accuracy: 92.3%
  ‚Ä¢ Macro F1-Score: 93.5%
  ‚Ä¢ Weighted F1-Score: 92.4%

Per-Class F1-Scores:
  ‚Ä¢ Class 0: 0.974 (n=196)
  ‚Ä¢ Class 1: 0.886 (n=329)
  ‚Ä¢ Class 2: 0.918 (n=492)
  ‚Ä¢ Class 3: 0.964 (n=141)

‚úì Results saved: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/tabnet/training_results_tabnet.json

[Step 8/9] Generating visualizations...
‚úì Confusion matrices saved
‚úì Optimization history saved: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/tabnet/tabnet_optuna_history.html

[Step 9/9] Analyzing feature importance through attention masks...

Top 10 Most Important Features (by attention selection):
------------------------------------------------------------
lat_ms                        :   0.2337
pdr                           :   0.1722
categoria                     :   0.1009
throughput_kbps_mean3         :   0.0721
pdr_mean3                     :   0.0678
throughput_bps                :   0.0623
loss_ratio                    :   0.0566
throughput_kbps               :   0.0409
app                           :   0.0298
vazao_env_carro_media_bps     :   0.0208

‚úì Feature importances saved: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/tabnet/feature_importances_tabnet.csv
‚úì Feature importance plot saved

================================================================================
TABNET TRAINING SUMMARY
================================================================================
‚úì Model Architecture: TabNet (n_d=16, n_a=8, steps=5)
‚úì Total Parameters: 21,179
‚úì Device: CUDA
‚úì Best CV Score: 0.931
‚úì Holdout Accuracy: 92.3%
‚úì Training completed in 20 trials
‚úì All artifacts saved to: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/tabnet
================================================================================
‚úì TabNet training completed

[3/3] Training CatBoost Classifier...
------------------------------------------------------------

================================================================================
CATBOOST TRAINING PIPELINE
================================================================================

Saving preprocessing artifacts for CatBoost...
‚úì Features saved: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/cb/X_cb.csv
‚úì Labels saved: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/cb/y_cb.csv (text) and y_cb.npy (binary)
‚úì Groups saved: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/cb/groups_cb.csv (text) and groups_cb.npy (binary)
‚úì Class weights saved: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/cb/class_weight_cb.json

üìÅ All artifacts saved to: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/cb

------------------------------------------------------------
HYPERPARAMETER OPTIMIZATION
------------------------------------------------------------
Best‚Äátrial:‚Äá36.‚ÄáBest‚Äávalue:‚Äá0.954126:‚Äá100%
‚Äá40/40‚Äá[05:56<00:00,‚Äá‚Äá9.76s/it]

Optimization completed!
Best hyperparameters found:
  depth: 8
  learning_rate: 0.20003664481897915
  l2_leaf_reg: 1.1867610593722853
  bootstrap_type: Bayesian
  bagging_temperature: 0.7068329287099645
Best F1-macro (CV): 0.954

------------------------------------------------------------
FINAL MODEL TRAINING
------------------------------------------------------------
‚úì Model saved: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/cb/catboost_best.pkl

------------------------------------------------------------
HOLDOUT SET EVALUATION
------------------------------------------------------------

Classification Report:
              precision    recall  f1-score   support

           0      0.990     0.964     0.977       196
           1      0.958     0.897     0.926       329
           2      0.929     0.978     0.952       492
           3      0.986     0.986     0.986       141

    accuracy                          0.953      1158
   macro avg      0.965     0.956     0.960      1158
weighted avg      0.954     0.953     0.953      1158


Key Performance Indicators:
  ‚Ä¢ Overall Accuracy: 95.3%
  ‚Ä¢ Macro F1-Score: 96.0%
  ‚Ä¢ Safety Class (0) F1: 97.7%
‚úì Results saved: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/cb/training_results_cb.json

------------------------------------------------------------
GENERATING VISUALIZATIONS
------------------------------------------------------------
‚úì Confusion matrices saved
‚úì Optimization history saved: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/cb/catboost_optuna_history.html

------------------------------------------------------------
FEATURE IMPORTANCE ANALYSIS
------------------------------------------------------------

Top 10 Most Important Features:
--------------------------------------------------
num__lat_ms                        :  27.5856
num__loss_ratio                    :  14.2754
num__pdr                           :  11.5363
num__pdr_mean3                     :  10.7280
num__lambda                        :   9.0658
num__throughput_bps                :   6.5053
num__throughput_kbps_mean3         :   3.9122
num__lat_ms_mean3                  :   3.0003
num__vazao_rec_servidor_media_bps  :   2.8791
cat__arquivo_origem_metricas_v10_ci_g_fn.csv:   1.5988

‚úì Feature importances saved: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/cb/feature_importances_cb.csv

================================================================================
CATBOOST TRAINING SUMMARY
================================================================================
‚úì Model Type: CatBoost with GPU acceleration
‚úì Best CV Score: 0.954
‚úì Holdout Accuracy: 95.3%
‚úì Training completed in 40 trials
‚úì All artifacts saved to: /content/drive/MyDrive/Colab Notebooks/vtm2025/artifacts/cb
================================================================================
‚úì CatBoost training completed

================================================================================
All models trained successfully!
Results saved in: drive/MyDrive/Colab Notebooks/vtm2025-git/artifacts/
================================================================================